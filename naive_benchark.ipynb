{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee670942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3cc235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_mape(y, yhat):\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    yhat = np.asarray(yhat, dtype=float)\n",
    "    mask = y != 0\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    return float(np.mean(np.abs((yhat[mask] - y[mask]) / y[mask])))\n",
    "\n",
    "\n",
    "def compute_naive_store_metrics(df, steps=54):\n",
    "    \"\"\"\n",
    "    df: long DataFrame with at least ['Store', 'Date', 'Sales']\n",
    "        assumed already filtered to the evaluation horizon (train+test)\n",
    "    steps: number of last days per store used as test set\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df.sort_values([\"Store\", \"Date\"], inplace=True)\n",
    "\n",
    "    store_metrics = []\n",
    "\n",
    "    for store_id, g in df.groupby(\"Store\"):\n",
    "        g = g.sort_values(\"Date\")\n",
    "        y = g[\"Sales\"].to_numpy(dtype=float)\n",
    "\n",
    "        if len(y) <= steps:\n",
    "            # not enough history to form train+test\n",
    "            continue\n",
    "\n",
    "        # last `steps` days are test\n",
    "        test_y = y[-steps:]\n",
    "        # naive prediction: yesterday's sales\n",
    "        # for each test day t, use y_{t-1}\n",
    "        naive_pred = y[-steps-1:-1]  # same length as test_y\n",
    "\n",
    "        err = naive_pred - test_y  # prediction - actual\n",
    "\n",
    "        rows = len(test_y)\n",
    "        sales_sum = float(test_y.sum())\n",
    "        abs_err = np.abs(err)\n",
    "        abs_err_sum = float(abs_err.sum())\n",
    "        sq_err_sum = float((err ** 2).sum())\n",
    "\n",
    "        mae = float(abs_err.mean())\n",
    "        rmse = float(np.sqrt(sq_err_sum / rows))\n",
    "        mape = _safe_mape(test_y, naive_pred)\n",
    "        wape = abs_err_sum / sales_sum if sales_sum != 0 else np.nan\n",
    "\n",
    "        tae = float(np.abs(naive_pred.sum() - test_y.sum()))\n",
    "        bias = float(err.mean())\n",
    "        err_std = float(err.std(ddof=0))\n",
    "\n",
    "        # for global MAPE (true point-wise weighted MAPE)\n",
    "        mask = test_y != 0\n",
    "        abs_pct_err_sum = float(np.abs(err[mask] / test_y[mask]).sum())\n",
    "        nonzero_n = int(mask.sum())\n",
    "\n",
    "        store_metrics.append({\n",
    "            \"Store\": store_id,\n",
    "            \"rows\": rows,\n",
    "            \"sales_sum\": sales_sum,\n",
    "            \"wape\": wape,\n",
    "            \"bias\": bias,\n",
    "            \"err_std\": err_std,\n",
    "            \"TAE\": tae,\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAPE\": mape,\n",
    "            \"abs_err_sum\": abs_err_sum,\n",
    "            \"sq_err_sum\": sq_err_sum,\n",
    "            \"abs_pct_err_sum\": abs_pct_err_sum,\n",
    "            \"nonzero_n\": nonzero_n,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(store_metrics)\n",
    "\n",
    "\n",
    "def summarize_org_metrics(store_df):\n",
    "    \"\"\"\n",
    "    store_df: output of compute_naive_store_metrics\n",
    "    Returns a dict with organization-level metrics comparable\n",
    "    to those used for SARIMAX/XGBoost.\n",
    "    \"\"\"\n",
    "    total_sales = store_df[\"sales_sum\"].sum()\n",
    "    total_abs_err = store_df[\"abs_err_sum\"].sum()\n",
    "    total_sq_err = store_df[\"sq_err_sum\"].sum()\n",
    "    total_rows = store_df[\"rows\"].sum()\n",
    "\n",
    "    # global WAPE (org-level)\n",
    "    org_wape = total_abs_err / total_sales if total_sales != 0 else np.nan\n",
    "\n",
    "    # \"Weighted RMSE\" as RMSE over all store-day observations\n",
    "    org_weighted_rmse = np.sqrt(total_sq_err / total_rows) if total_rows > 0 else np.nan\n",
    "\n",
    "    # unweighted MAPE = simple average of store-level MAPEs\n",
    "    org_mape_unweighted = store_df[\"MAPE\"].mean()\n",
    "\n",
    "    # global point-wise MAPE (true weighted by number of nonzero points)\n",
    "    total_abs_pct_err = store_df[\"abs_pct_err_sum\"].sum()\n",
    "    total_nonzero = store_df[\"nonzero_n\"].sum()\n",
    "    org_mape_weighted = (total_abs_pct_err / total_nonzero) if total_nonzero > 0 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"WAPE\": org_wape,\n",
    "        \"Weighted_RMSE\": org_weighted_rmse,\n",
    "        \"MAPE_weighted\": org_mape_weighted,\n",
    "        \"MAPE_unweighted\": org_mape_unweighted,\n",
    "        \"Total_sales\": total_sales,\n",
    "        \"Total_rows\": total_rows,\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
